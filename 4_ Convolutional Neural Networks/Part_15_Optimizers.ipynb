{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Part_15_Optimizers.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"QfdCmmtSWNZ_","colab_type":"text"},"source":["## Summer School on Deep Learning Surathkal, Karnataka - 2019\n","## Optimizers\n","* https://keras.io/optimizers/"]},{"cell_type":"markdown","metadata":{"id":"wJ7_MxBZWQYO","colab_type":"text"},"source":["### Variations of Gradient Descent\n","* Stachastic Gradient Descent\n","* RMSprop\n","* Adagrad\n","* Adadelta\n","* Adam\n","* Adamax\n","* Nadam"]},{"cell_type":"markdown","metadata":{"id":"QZvZqcdzkbcd","colab_type":"text"},"source":["### Stachastic Gradient Descent\n","#### Arguments\n","* lr: float >= 0. Learning rate.\n","* Momentum: float >= 0. Parameter that accelerates SGD in the relevant direction and dampens oscillations.\n","* Decay: float >= 0. Learning rate decay over each update.\n","* Nesterov: boolean, whether to apply Nesterov momentum.\n"]},{"cell_type":"code","metadata":{"id":"t8ZAK2j4WGak","colab_type":"code","outputId":"a18e77c0-7dcf-4742-e4ff-347104f838a8","executionInfo":{"status":"ok","timestamp":1557485249722,"user_tz":-330,"elapsed":1232,"user":{"displayName":"SummerSchool DeepLearning","photoUrl":"","userId":"02511657062467111974"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["import keras\n","from keras import optimizers\n","from keras.models import Sequential\n","from keras.layers import *\n","\n","model = Sequential()\n","model.add(Dense(64, kernel_initializer='uniform', input_shape=(10,)))\n","model.add(Activation('softmax'))\n","\n","sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n","model.compile(loss='mean_squared_error', optimizer=sgd)\n","model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_2 (Dense)              (None, 64)                704       \n","_________________________________________________________________\n","activation_2 (Activation)    (None, 64)                0         \n","=================================================================\n","Total params: 704\n","Trainable params: 704\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"r_5ErBYWk_iz","colab_type":"text"},"source":["### RMSprop\n","####  Arguments\n","* lr: float >= 0. Learning rate.\n","* rho: float >= 0.\n","* epsilon: float >= 0. Fuzz factor. If None, defaults to K.epsilon().\n","* decay: float >= 0. Learning rate decay over each update."]},{"cell_type":"code","metadata":{"id":"mkCZcE-llPvc","colab_type":"code","outputId":"05b044ce-290a-4dc4-b9a5-677ab31c074f","executionInfo":{"status":"ok","timestamp":1557505889849,"user_tz":-330,"elapsed":1242,"user":{"displayName":"SummerSchool DeepLearning","photoUrl":"","userId":"02511657062467111974"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["import keras\n","from keras import optimizers\n","from keras.models import Sequential\n","from keras.layers import *\n","\n","model = Sequential()\n","model.add(Dense(64, kernel_initializer='uniform', input_shape=(10,)))\n","model.add(Activation('softmax'))\n","\n","rms = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n","model.compile(loss='mean_squared_error', optimizer=rms )\n","model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_2 (Dense)              (None, 64)                704       \n","_________________________________________________________________\n","activation_2 (Activation)    (None, 64)                0         \n","=================================================================\n","Total params: 704\n","Trainable params: 704\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Pg1Aow57leP3","colab_type":"text"},"source":["### Adagrad\n","#### Arguments\n","* Lr: float >= 0. Initial learning rate.\n","* epsilon: float >= 0. If None, defaults to K.epsilon().\n","* decay: float >= 0. Learning rate decay over each update\n"]},{"cell_type":"code","metadata":{"id":"nxHyuqWXlkV7","colab_type":"code","outputId":"31e388b9-ca8e-4fd8-e9c6-f6eceee9c8f2","executionInfo":{"status":"ok","timestamp":1557506576060,"user_tz":-330,"elapsed":1204,"user":{"displayName":"SummerSchool DeepLearning","photoUrl":"","userId":"02511657062467111974"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["import keras\n","from keras import optimizers\n","from keras.models import Sequential\n","from keras.layers import *\n","\n","model = Sequential()\n","model.add(Dense(64, kernel_initializer='uniform', input_shape=(10,)))\n","model.add(Activation('softmax'))\n","\n","adagrad = keras.optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n","model.compile(loss='mean_squared_error', optimizer=adagrad  )\n","model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_6 (Dense)              (None, 64)                704       \n","_________________________________________________________________\n","activation_6 (Activation)    (None, 64)                0         \n","=================================================================\n","Total params: 704\n","Trainable params: 704\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KgQqg6zkmX3H","colab_type":"text"},"source":["### Adadelta\n","#### Arguments\n","* lr: float >= 0. Initial learning rate, defaults to 1. It is recommended to leave it at the default value.\n","* rho: float >= 0. Adadelta decay factor, corresponding to fraction of gradient to keep at each time step.\n","* epsilon: float >= 0. Fuzz factor. If None, defaults to K.epsilon().\n","* decay: float >= 0. Initial learning rate decay."]},{"cell_type":"code","metadata":{"id":"-V0LZRK2mqqW","colab_type":"code","outputId":"031fac26-ea9e-47cd-ed4b-6f3012b1f1b5","executionInfo":{"status":"ok","timestamp":1557506548227,"user_tz":-330,"elapsed":1250,"user":{"displayName":"SummerSchool DeepLearning","photoUrl":"","userId":"02511657062467111974"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["import keras\n","from keras import optimizers\n","from keras.models import Sequential\n","from keras.layers import *\n","\n","model = Sequential()\n","model.add(Dense(64, kernel_initializer='uniform', input_shape=(10,)))\n","model.add(Activation('softmax'))\n","\n","adadelta = keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n","model.compile(loss='mean_squared_error', optimizer=ada )\n","model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_4 (Dense)              (None, 64)                704       \n","_________________________________________________________________\n","activation_4 (Activation)    (None, 64)                0         \n","=================================================================\n","Total params: 704\n","Trainable params: 704\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Gx32GTD1oHXC","colab_type":"text"},"source":["### Adam\n","#### Arguments\n","* lr: float >= 0. Learning rate.\n","* beta_1: float, 0 < beta < 1. Generally close to 1.\n","* beta_2: float, 0 < beta < 1. Generally close to 1.\n","* epsilon: float >= 0. Fuzz factor. If None, defaults to K.epsilon().\n","* decay: float >= 0. Learning rate decay over each update.\n","* amsgrad: boolean. Whether to apply the AMSGrad variant of this algorithm from the paper \"On the Convergence of Adam and Beyond\"."]},{"cell_type":"code","metadata":{"id":"gk-47hNpog3B","colab_type":"code","outputId":"7ae6b109-b971-47a0-a708-44ff8bb0d09c","executionInfo":{"status":"ok","timestamp":1557506712346,"user_tz":-330,"elapsed":1242,"user":{"displayName":"SummerSchool DeepLearning","photoUrl":"","userId":"02511657062467111974"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["import keras\n","from keras import optimizers\n","from keras.models import Sequential\n","from keras.layers import *\n","\n","model = Sequential()\n","model.add(Dense(64, kernel_initializer='uniform', input_shape=(10,)))\n","model.add(Activation('softmax'))\n","\n","adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n","model.compile(loss='mean_squared_error', optimizer=ada )\n","model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_7 (Dense)              (None, 64)                704       \n","_________________________________________________________________\n","activation_7 (Activation)    (None, 64)                0         \n","=================================================================\n","Total params: 704\n","Trainable params: 704\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"z5UHoGxXo_Cp","colab_type":"text"},"source":["### Adamax (Nesterov Adam optimizer)\n","####Arguments\n","* lr: float >= 0. Learning rate.\n","* beta_1: floats, 0 < beta < 1. Generally close to 1.\n","* beta_2: floats, 0 < beta < 1. Generally close to 1.\n","* epsilon: float >= 0. Fuzz factor. If None, defaults to K.epsilon().\n","* bdecay: float >= 0. Learning rate decay over each update."]},{"cell_type":"code","metadata":{"id":"d_N9XzpwpPFS","colab_type":"code","outputId":"74ce494d-7170-40fd-fe6b-c56ee5f5facf","executionInfo":{"status":"ok","timestamp":1557506903108,"user_tz":-330,"elapsed":1275,"user":{"displayName":"SummerSchool DeepLearning","photoUrl":"","userId":"02511657062467111974"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["import keras\n","from keras import optimizers\n","from keras.models import Sequential\n","from keras.layers import *\n","\n","model = Sequential()\n","model.add(Dense(64, kernel_initializer='uniform', input_shape=(10,)))\n","model.add(Activation('softmax'))\n","\n","adamx = keras.optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n","model.compile(loss='mean_squared_error', optimizer=ada )\n","model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_8 (Dense)              (None, 64)                704       \n","_________________________________________________________________\n","activation_8 (Activation)    (None, 64)                0         \n","=================================================================\n","Total params: 704\n","Trainable params: 704\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lY_ThgBjpXKx","colab_type":"text"},"source":["### Nadam\n","#### Arguments\n","* lr: float >= 0. Learning rate.\n","* beta_1: floats, 0 < beta < 1. Generally close to 1.\n","* beta_2: floats, 0 < beta < 1. Generally close to 1.\n","* epsilon: float >= 0. Fuzz factor. If None, defaults to K.epsilon().\n","* schedule_decay: floats, 0 < schedule_decay < 1."]},{"cell_type":"code","metadata":{"id":"_pjhuHZ8pjsE","colab_type":"code","outputId":"fcd2bb1b-a66b-4333-b639-f2f67cab9911","executionInfo":{"status":"ok","timestamp":1557506984826,"user_tz":-330,"elapsed":1265,"user":{"displayName":"SummerSchool DeepLearning","photoUrl":"","userId":"02511657062467111974"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["import keras\n","from keras import optimizers\n","from keras.models import Sequential\n","from keras.layers import *\n","\n","model = Sequential()\n","model.add(Dense(64, kernel_initializer='uniform', input_shape=(10,)))\n","model.add(Activation('softmax'))\n","\n","nadam = keras.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n","model.compile(loss='mean_squared_error', optimizer=ada )\n","model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_9 (Dense)              (None, 64)                704       \n","_________________________________________________________________\n","activation_9 (Activation)    (None, 64)                0         \n","=================================================================\n","Total params: 704\n","Trainable params: 704\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]}]}